{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping and REST APIs \n",
    "\n",
    "This tutorial is a part of the [Zero to Data Analyst Bootcamp by Jovian](https://www.jovian.ai/data-analyst-bootcamp)\n",
    "\n",
    "![](https://i.imgur.com/6zM7JBq.png)\n",
    "\n",
    "\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning. While web scraping often involves parsing and processing [HTML documents](https://developer.mozilla.org/en-US/docs/Web/HTML), some platforms also offer [REST APIs](https://www.smashingmagazine.com/2018/01/understanding-using-rest-api/) to retrieve information in a machine-readable format like [JSON](https://www.digitalocean.com/community/tutorials/an-introduction-to-json). In this tutorial, we'll use web scraping and REST APIs to create a real-world dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers the following topics: \n",
    "\n",
    "* Downloading web pages using the requests library\n",
    "* Inspecting the HTML source code of a web page\n",
    "* Parsing parts of a website using Beautiful Soup\n",
    "* Writing parsed information into CSV files\n",
    "* Using a REST API to retrieve data as JSON\n",
    "* Combining data from multiple sources\n",
    "* Using links on a page to crawl a website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run the Code\n",
    "\n",
    "The best way to learn the material is to execute the code and experiment with it yourself. This tutorial is an executable [Jupyter notebook](https://jupyter.org). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
    "\n",
    "#### Option 1: Running using free online resources (1-click, recommended)\n",
    "\n",
    "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Binder**. You can also select \"Run on Colab\" or \"Run on Kaggle\", but you'll need to create an account on [Google Colab](https://colab.research.google.com) or [Kaggle](https://kaggle.com) to use these platforms.\n",
    "\n",
    "\n",
    "#### Option 2: Running on your computer locally\n",
    "\n",
    "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem \n",
    "\n",
    "Over the course of this tutorial, we'll solve the following problem to learn the tools and techniques used for web scraping:\n",
    "\n",
    "\n",
    "> **QUESTION**: Write a Python function that creates a CSV file (comma-separated values) containing details about the 25 top GitHub repositories for any given topic. You can view the top repositories for the topic `machine-learning` on this page: [https://github.com/topics/machine-learning](https://github.com/topics/machine-learning). The output CSV should contain these details: repository name, owner's username, no. of stars, repository URL. \n",
    "\n",
    "\n",
    " <a href=\"https://github.com/topics/machine-learning\"><img src=\"https://i.imgur.com/5V1HGLs.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\"></a>\n",
    " \n",
    " \n",
    "How would you go about solving this problem in Python? Explore the web page and take a couple of minutes to come up with an approach before proceeding further. How many lines of code do you think the solution will require?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading a web page using `requests`\n",
    "\n",
    "When you access a URL like https://github.com/topics/machine-learning using a web browser, it downloads the contents of the web page the URL points to and displays the output on the screen. Before we can extract information from a web page, we need to download the page using Python.\n",
    "\n",
    "We'll use a library called [`requests`](https://docs.python-requests.org/en/master/) to download web pages from the internet. Let's begin by installing and importing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download a web page using the `requests.get` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = 'https://github.com/topics/machine-learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests.get` returns a response object with the page contents and some information indicating whether the request was successful, using a status code. Learn more about HTTP status codes here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status. \n",
    "\n",
    " If the request was successful, `response.status_code` is set to a value between 200 and 299. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the web page can be accessed using the `.text` property of the `response`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603952"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contains over 60,000 characters! Let's view the first 1000 characters of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-PYWr2OavT8crCvolPhJe+bHZ6PG6Q6cH7+2eZue+suNLa9t4w/spUoiSCNG+JfpZIL7kq9rnGXwNXCJup7IQdA==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-3d85abd8e6af4fc72b0afa253e125ef9.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-jaRxAk/R7Eq6XXtxt2dWYc6UfgT/Jk9zYWYh4UpAt5LFRnYVaWqEM3sPhUFL3fOBmHhHoOcn4wfLkMS21Q1yaw==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/site-8da471024fd1ec4aba5d7b71b7675661.css\" />\\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-jTdvoiCezBiH9yw26ZDI7d23d6fazvCUVOTMSiazFi9A'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see above is the *source code* of the web page. It written in a language called [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML). It defines the content and structure of the web page. \n",
    "\n",
    "Let's save the contents to a file with the `.html` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('machine-learning-topics.html', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now view the file using the \"File > Open\" menu option within Jupyter and clicking on *machine-learning.html* in the list of files displayed. Here's what you'll see when you open the file:\n",
    "\n",
    "<img src=\"https://i.imgur.com/8gEbT1P.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "While this looks similar to the original web page, note that it's simply a copy. You will notice that none of the links or buttons work. To view or edit the source code of the file, click \"File > Open\" within Jupyter, then select the file *machine-learning.html* from the list and click the \"Edit\" button.\n",
    "\n",
    "<img src=\"https://i.imgur.com/JG7Q8CK.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "As you might expect, the source code looks something like this:\n",
    "\n",
    "<img src=\"https://i.imgur.com/6ynXNdz.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "Try scrolling through the source code. Can you make sense of it? Can you see how the information on the page is organized within the file? We'll learn more about it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Download the web page for a different topic, e.g., https://github.com/topics/data-analysis using `requests` and save it to a file, e.g., `data-analysis.html`. View the page and compare it with the previously downloaded page? How are the two different? Can you spot the differences in the source code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work using `jovian` before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project='python-web-scraping-and-rest-api')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the HTML source code of a web page\n",
    "\n",
    "![](https://i.imgur.com/mvBpQIP.png)\n",
    "\n",
    "As mentioned earlier, web pages are written in a language called HTML (Hyper Text Markup Language). HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://jovian.ai\" target=\"_blank\">Go to Jovian</a>`. An HTML tag has three parts:\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside an HTML Document\n",
    "\n",
    "Here's a simple HTML document that uses many commonly used tags:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>All About Python</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div style=\"width: 640px; margin: 40px auto\">\n",
    "      <h1 style=\"text-align:center;\">Python - A Programming Language</h1>\n",
    "      <img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" alt=\"python-logo\" style=\"width:240px;margin:0 auto;display:block;\">\n",
    "      <div>\n",
    "        <h2>About Python</h2>\n",
    "        <p>\n",
    "          Python is an <span style=\"font-style: italic\">interpreted, high-level and general-purpose</span> programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Visit the <a href=\"https://docs.python.org/3/\">official documentation</a> to learn more.\n",
    "        </p>\n",
    "      </div>\n",
    "      <div>\n",
    "        <h2>Some Python Libraries</h2>\n",
    "        <ul id=\"libraries\">\n",
    "          <li>Numpy</li>\n",
    "          <li>Pandas</li>\n",
    "          <li>PyTorch</li>\n",
    "          <li>Scikit Learn</li>\n",
    "        </ul>\n",
    "      </div>\n",
    "      <div>\n",
    "        <h2>Recent Python Versions</h2>\n",
    "        <table id=\"versions-table\">\n",
    "          <tr>\n",
    "            <th class=\"bordered-table\">Version</th>\n",
    "            <th class=\"bordered-table\">Released on</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td class=\"bordered-table\">Python 3.8</td>\n",
    "            <td class=\"bordered-table\">October 2019</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td class=\"bordered-table\">Python 3.7</td>\n",
    "            <td class=\"bordered-table\">June 2018</td>\n",
    "          </tr>\n",
    "        </table>\n",
    "          <style>\n",
    "              .bordered-table { \n",
    "                  border: 1px solid black; padding: 8px;\n",
    "              }\n",
    "          </style>\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "```\n",
    "\n",
    "> **EXERCISE**: Copy the above HTML code and paste it into a new file called `webpage.html`. To create a new file,  select \"File > Open\" from the menu bar, then select \"New > Text\" file. View the saved file. Can you see how the different tags are displayed in different ways by the browser?\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/lcSHz5V.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "> **EXERCISE**: Make some changes to the code inside `webpage.html`. Save the file and view it again. Do you see your changes reflected? Play with the structure of the file. Try to break things and fix them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Tags and Attributes\n",
    "\n",
    "Following are some of the most commonly used HTML tags:\n",
    "\n",
    "* `html`\n",
    "* `head`\n",
    "* `title`\n",
    "* `body`\n",
    "* `div`\n",
    "* `span`\n",
    "* `h1` to `h6`\n",
    "* `p`\n",
    "* `img`\n",
    "* `ul`, `ol` and `li`\n",
    "* `table`, `tr`, `th` and `td`\n",
    "* `style`\n",
    "* ...\n",
    "\n",
    "Each tag supports several attributes. Following are some common attributes used to modify the behavior of tags:\n",
    "\n",
    "* `id`\n",
    "* `style`\n",
    "* `class`\n",
    "* `href` (used with `<a>`)\n",
    "* `src` (used with `<img>`)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> **EXERCISE**: Complete this tutorial on HTML: https://www.htmldog.com/guides/html/ . Once done, try describing what the above tags and attributes are used for. Try creating a new HTML page using the tags you find most interesting. \n",
    "> \n",
    "> To learn how to style HTML tags, check out this tutorial on CSS: https://www.htmldog.com/guides/css/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting HTML in the Browser\n",
    "\n",
    "You can view the source code of any webpage right within your browser by right-clicking anywhere on a page and selecting the \"Inspect\" option. It opens the \"Developer Tools\" pane, where you can see the source code as a tree. You can expand and collapse various nodes and find the source code for a specific portion of the page.\n",
    "\n",
    "Here's what it looks like on the Chrome browser:\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/jCA1T6Z.png\" width=\"640\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "\n",
    "> **EXERCISE**: Explore the source code of the web page https://github.com/topics/machine-learning . Try to find the portions in the source code corresponding to the repository name, owner's username, and the number of stars for each repository listed on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information from HTML using Beautiful Soup\n",
    "\n",
    "To extract information from the HTML source code of a webpage programmatically, we can use the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library. Let's install the library and import the `BeautifulSoup` class from the `bs4` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "?BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read the contents of the file `machine-learning.html` and create a `BeautifulSoup` object to parse the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('machine-learning-topics.html', 'r') as f:\n",
    "    html_source = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-PYWr2OavT8crCvolPhJe+bHZ6PG6Q6cH7+2eZue+suNLa9t4w/spUoiSCNG+JfpZIL7kq9rnGXwNXCJup7IQdA==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-3d85abd8e6af4fc72b0afa253e125ef9.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-jaRxAk/R7Eq6XXtxt2dWYc6UfgT/Jk9zYWYh4UpAt5LFRnYVaWqEM3sPhUFL3fOBmHhHoOcn4wfLkMS21Q1yaw==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/site-8da471024fd1ec4aba5d7b71b7675661.css\" />\\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-jTdvoiCezBiH9yw26ZDI7d23d6fazvCUVOTMSiazFi9A'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_source[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(html_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` object contains several properties and methods for extracting information from the HTML document. Let's look at a few examples below.\n",
    "\n",
    "**NOTE**: You don't need to remember all (or any) of the properties/methods. You can look up [the documentation of BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) or [search online](https://www.google.co.in/search?q=beautifulsoup+how+to+get+href+of+link) to find what you need when you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing a tag\n",
    "\n",
    "> **QUESTION**: Find the title of the page represented by `doc`.\n",
    "\n",
    "The title of the page is contained within the `<title>` tag. We can access the title tag using `doc.title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag = doc.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>machine-learning · GitHub Topics · GitHub</title>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(title_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a tag's name using the `.name` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text within a tag can be accessed using `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine-learning · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Explore the `html`, `body`, and `head` tags of `doc`. Do you see what you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tag occurs more than once in a document e.g. `<a>` (which represents links), then `doc.a` finds the first `<a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = doc.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to content'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the first occurrence of each of these tags in `doc`: `div`, `img`, `span`, `p`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all tags of the same type\n",
    "\n",
    "To find all the occurrences of a tag, use the `find_all` method.\n",
    "\n",
    "> **QUESTION**: Find all the link tags on the page. How many links does the page contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_tags = doc.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_link_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>,\n",
       " <a aria-label=\"Homepage\" class=\"mr-4\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\" href=\"https://github.com/\">\n",
       " <svg aria-hidden=\"true\" class=\"octicon octicon-mark-github color-text-white\" height=\"32\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"32\"><path d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\" fill-rule=\"evenodd\"></path></svg>\n",
       " </a>,\n",
       " <a class=\"d-inline-block d-lg-none f5 color-text-white no-underline border color-border-tertiary rounded-2 px-2 py-1 mr-3 mr-sm-5 js-signup-redesign-control js-signup-redesign-target\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"e755e218b5b5c8ea0dcf7c1afea104ce5b0e7bb40929d477942f46cf5b206533\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "                 Sign up\n",
       "               </a>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_link_tags[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Get a list of all the `img` tags on the page. How many images does the page contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing attributes\n",
    "\n",
    "The attributes of a tag can be accessed using the indexing notation, e.g., `first_link['href']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#start-of-content'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['px-2',\n",
       " 'py-4',\n",
       " 'color-bg-info-inverse',\n",
       " 'color-text-white',\n",
       " 'show-on-focus',\n",
       " 'js-skip-to-content']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `class` attribute is automatically split into a list of classes (this isn't done for any other attribute). This is because it's common practice to check for a specific class within a tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `.attrs` property to view all the attributes as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '#start-of-content',\n",
       " 'class': ['px-2',\n",
       "  'py-4',\n",
       "  'color-bg-info-inverse',\n",
       "  'color-text-white',\n",
       "  'show-on-focus',\n",
       "  'js-skip-to-content']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the 5th image tag on the page (counting from 0). Which attributes does the tag contain? Find the values of the `src` and `alt` attributes of the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching by Attribute Value\n",
    "\n",
    "> **QUESTION**: Find the `img` tag(s) on the page with the `alt` attribute set to `tsbertalan`.\n",
    "\n",
    "We can provide a dictionary of attributes as the second argument to `find_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"tsbertalan\" class=\"avatar avatar-user avatar-small\" height=\"32\" src=\"https://avatars.githubusercontent.com/u/306137?v=4\" width=\"32\"/>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find_all('img', { 'alt': 'tsbertalan'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're just interested in the first element, we can use the `find` method. Keep in mind that `find` returns `None` if no matching tag is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"tsbertalan\" class=\"avatar avatar-user avatar-small\" height=\"32\" src=\"https://avatars.githubusercontent.com/u/306137?v=4\" width=\"32\"/>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('img', { 'alt': 'tsbertalan'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the `src` attribute of the first `img` tag with the `alt` attribute set to `julia`. Visit the link and check what the image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching by Class\n",
    "\n",
    "The `class` attribute is one of the most frequently used attributes on HTML tags (used for layout and styling). We can search for tags containing a class using the `class_` argument in `find_all` (note that `class` is a reserved keyword in Python, hence the underscore in the argument name).\n",
    "\n",
    "> **QUESTION**: Find all the tags containing the class `HeaderMenu-link`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_tags = doc.find_all(class_='HeaderMenu-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Why GitHub?\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Team\" href=\"/team\">Team</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Enterprise\" href=\"/enterprise\">Enterprise</a>,\n",
       " <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Explore\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Marketplace\" href=\"/marketplace\">Marketplace</a>,\n",
       " <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Pricing\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 no-underline mr-3\" data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header menu\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"77fa0805c2ee5e083e5dbe5571a2e37d8661eca3a115806e97d54914b8332ea9\" href=\"/login?return_to=%2Ftopics%2Fmachine-learning\">\n",
       "           Sign in\n",
       "         </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded px-2 py-1 js-signup-redesign-target js-signup-redesign-control\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded-1 px-2 py-1 js-signup-redesign-target js-signup-redesign-variation\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" hidden=\"\" href=\"/join_next?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also for a specific type of tag e.g. `<a>` matching the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_link_tags = doc.find_all('a', class_='HeaderMenu-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Team\" href=\"/team\">Team</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Enterprise\" href=\"/enterprise\">Enterprise</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Marketplace\" href=\"/marketplace\">Marketplace</a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 no-underline mr-3\" data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header menu\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"77fa0805c2ee5e083e5dbe5571a2e37d8661eca3a115806e97d54914b8332ea9\" href=\"/login?return_to=%2Ftopics%2Fmachine-learning\">\n",
       "           Sign in\n",
       "         </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded px-2 py-1 js-signup-redesign-target js-signup-redesign-control\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded-1 px-2 py-1 js-signup-redesign-target js-signup-redesign-variation\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" hidden=\"\" href=\"/join_next?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_link_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Information from Tags\n",
    "\n",
    "Once we have a list of tags matching some criteria, it's easy to extract information and convert it to a more convenient format.\n",
    "\n",
    "> **QUESTION**: Find the link text and URL of all the links withing the page header on https://github.com/topics/machine-learning .\n",
    "\n",
    "We'll create a list of dictionaries containing the required information. We'll add the base URL https://github.com as a prefix because the `href` attribute only contains the relative path e.g. `/explore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/team'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_link_tags[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Team', 'url': 'https://github.com/team'},\n",
       " {'title': 'Enterprise', 'url': 'https://github.com/enterprise'},\n",
       " {'title': 'Marketplace', 'url': 'https://github.com/marketplace'},\n",
       " {'title': 'Sign in',\n",
       "  'url': 'https://github.com/login?return_to=%2Ftopics%2Fmachine-learning'},\n",
       " {'title': 'Sign up',\n",
       "  'url': 'https://github.com/join?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2Ftopics%2Fmachine-learning&source=header'},\n",
       " {'title': 'Sign up',\n",
       "  'url': 'https://github.com/join_next?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2Ftopics%2Fmachine-learning&source=header'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_links = []\n",
    "base_url = 'https://github.com'\n",
    "\n",
    "for tag in header_link_tags:\n",
    "    header_links.append({ 'title': tag.text.strip(), 'url': base_url + tag['href']})\n",
    "    \n",
    "header_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully extracted the required information about links in the page header. This is precisely what web scraping is: downloading a webpage, parsing the HTML, and extracting useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the list of all the images matching the class `avatar-user`. Each list element should be a dictionary containing two keys, `\"username\"` and `\"url\"`. You can obtain the username using the `alt` attribute of a tag and the URL using the `src` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements inside a tag\n",
    "\n",
    "> **QUESTION**: Find the `li` tags that are direct children of `ul` tag with the class `top-list` in the sample HTML document below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul class=\"top-list\">\n",
    "            <li>Item 1</li>\n",
    "            <li>Item 2</li>\n",
    "            <li>\n",
    "                <ul>\n",
    "                    <li>Item 3.1</li>\n",
    "                    <li>Item 3.2</li>\n",
    "                    <li>Item 3.3</li>\n",
    "                </ul> \n",
    "            </li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = BeautifulSoup(sample_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tag = sample_doc.find('ul', class_='top-list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `find_all` method on the tag, and set `recursive=False` to find just the direct children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_item_tags = list_tag.find_all('li', recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Item 1</li>,\n",
       " <li>Item 2</li>,\n",
       " <li>\n",
       " <ul>\n",
       " <li>Item 3.1</li>\n",
       " <li>Item 3.2</li>\n",
       " <li>Item 3.3</li>\n",
       " </ul>\n",
       " </li>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_item_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without `recursive=False`, the inner list items are also included in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Item 1</li>,\n",
       " <li>Item 2</li>,\n",
       " <li>\n",
       " <ul>\n",
       " <li>Item 3.1</li>\n",
       " <li>Item 3.2</li>\n",
       " <li>Item 3.3</li>\n",
       " </ul>\n",
       " </li>,\n",
       " <li>Item 3.1</li>,\n",
       " <li>Item 3.2</li>,\n",
       " <li>Item 3.3</li>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tag.find_all('li')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you don't need to remember all (or any) of the methods or properties offered by Beautiful Soup documents and tags. You should be able to figure out what you need to do, when you need to do it. Here's how:\n",
    "\n",
    "* Look up the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* Google what you're trying to do: https://www.google.co.in/search?q=beautiful+soup+get+href\n",
    "* Ask a question on StackOverflow: https://stackoverflow.com/questions/tagged/beautifulsoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Repositories for a Topic\n",
    "\n",
    "Let's return to our original problem statement of finding the top repositories for a given topic. Before we parse a page and find the top repositories, let's define a helper function to get the web page for any topic.\n",
    "\n",
    "> **QUESTION**: Define a function `get_topic_page` that downloads the GitHub web page for a given topic and returns a beautiful soup document representing the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic):\n",
    "    # Construct the URL\n",
    "    topic_repos_url = 'https://github.com/topics/' + topic\n",
    "    \n",
    "    # Get the HTML page content using requests\n",
    "    response = requests.get(topic_repos_url)\n",
    "    \n",
    "    # Ensure that the reponse is valid\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + topic_repos_url)\n",
    "    \n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('machine-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine-learning · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the topic page for another topic is now as simple as invoking the function with a different argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = get_topic_page('data-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data-analysis · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **QUESTION**: Develop an approach to find the repository name, owner's username, no. of stars, and repository link for the repositories listed on a topic page.\n",
    "\n",
    "<img src=\"https://i.imgur.com/szL76cU.png\" width=\"640\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "Upon inspecting the box containing the information for a repository, you will find an `article` tag for each repository, with `class` attribute set to  `border rounded color-shadow-small color-bg-secondary my-4`.\n",
    "\n",
    "Let's find all the `article` tags matching this class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 repositories listed on the page, and our query resulted in 30 article tags. It looks like we've found the enclosing tag for each repository. \n",
    "\n",
    "We need to extract the following information from each tag:\n",
    "\n",
    "1. Repository name\n",
    "2. Owner's username\n",
    "3. Number of stars\n",
    "4. Repository link\n",
    "\n",
    "Look at the source of any of the article tags. You will notice that the repository name, owner's username, and the repository link are all part of an `h1` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tag = article_tags[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to view\n",
    "# article_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the first `h1` inside an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"f3 color-text-secondary text-normal lh-condensed\">\n",
       "<a data-ga-click=\"Explore, go to repository owner, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"OWNER\",\"click_visual_representation\":\"REPOSITORY_OWNER_HEADING\",\"actor_id\":null,\"record_id\":10386605,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"1bae99446056a085802d3401cc23faad491e91eb64e36a9fbba9ac02ba2bb434\" href=\"/aymericdamien\">\n",
       "            aymericdamien\n",
       "</a>          /\n",
       "          <a class=\"text-bold\" data-ga-click=\"Explore, go to repository, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"REPOSITORY\",\"click_visual_representation\":\"REPOSITORY_NAME_HEADING\",\"actor_id\":null,\"record_id\":45986162,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"fef3fe4409b1dde6f30a7dba8f66011bd9736148681961664451736c9328262d\" href=\"/aymericdamien/TensorFlow-Examples\">\n",
       "            TensorFlow-Examples\n",
       "</a> </h1>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_tag = article_tag.find('h1')\n",
    "h1_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `h1` has `a` tags inside it, one containing the owner's username and the second containing the repository title. The `href` of the second tag also includes the relative path of the repository. Let's extract this information from the `a` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = h1_tag.find_all('a', recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            aymericdamien\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = a_tags[0].text\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the username contains some leading and trailing whitespace. We can get rid of it using `strip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aymericdamien'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = a_tags[0].text.strip()\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the repository name and repository path in the same fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TensorFlow-Examples'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = a_tags[1].text.strip()\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aymericdamien/TensorFlow-Examples'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = a_tags[1]['href'].strip()\n",
    "repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the full URL to the repository, we can append the base URL `https://github.com` at the beginning of the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/aymericdamien/TensorFlow-Examples'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://github.com'\n",
    "repo_url = base_url + repo_path \n",
    "repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, to get the number of starts, we notice that it is contained within an `a` tag which has the count `social-count float-none`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_star_tag = article_tags[4].find('a', class_='social-count float-none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"social-count float-none\" data-ga-click=\"Explore, go to repository stargazers, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"STARGAZERS\",\"click_visual_representation\":\"STARGAZERS_NUMBER\",\"actor_id\":null,\"record_id\":45986162,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"a46079c0b03997f1f8a4f276ba3457dbf1123db76023ef8d1a4d701ec64b17da\" href=\"/aymericdamien/TensorFlow-Examples/stargazers\">\n",
       "          40.4k\n",
       "</a>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_star_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the star count from the `a` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40.4k'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_star_tag.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `k` at the end indicates `1000`. Let's write a helper function which can convert strings like `40.3k` into the number `40,300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    else:\n",
    "        return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40300"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count('40.3k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count('991')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine the star count as a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_count = parse_star_count(a_star_tag.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40400"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, we've extracted all the information we were interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository name: TensorFlow-Examples\n",
      "Owner's username: aymericdamien\n",
      "Stars: 40400\n",
      "Repository URL: https://github.com/aymericdamien/TensorFlow-Examples\n"
     ]
    }
   ],
   "source": [
    "print('Repository name:', repo_name)\n",
    "print(\"Owner's username:\", username)\n",
    "print('Stars:', star_count)\n",
    "print('Repository URL:', repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the logic for parsing the required information from an article tag into a function.\n",
    "\n",
    "> **QUESTION**: Write a function `parse_repostory` that returns a dictionary containing the repository name, owner's username, number of stars, and repository URL by parsing a given `article` tag representing a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_repository(article_tag):\n",
    "    # <a> tags containing username, repository name and URL\n",
    "    a_tags = article_tag.h1.find_all('a')\n",
    "    # Owner's username\n",
    "    username = a_tags[0].text.strip()\n",
    "    # Repository name\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    # Repository URL\n",
    "    repo_url = base_url + a_tags[1]['href'].strip()\n",
    "    # Star count\n",
    "    stars_tag = article_tag.find('a', class_='social-count float-none')\n",
    "    star_count = parse_star_count(stars_tag.text.strip())\n",
    "    # Return a dictionary\n",
    "    return {\n",
    "        'repository_name': repo_name,\n",
    "        'owner_username': username,        \n",
    "        'stars': star_count,\n",
    "        'repository_url': repo_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function to parse any `article` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'tensorflow',\n",
       " 'owner_username': 'tensorflow',\n",
       " 'stars': 155000,\n",
       " 'repository_url': 'https://github.com/tensorflow/tensorflow'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_repository(article_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'awesome-scalability',\n",
       " 'owner_username': 'binhnguyennus',\n",
       " 'stars': 31600,\n",
       " 'repository_url': 'https://github.com/binhnguyennus/awesome-scalability'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_repository(article_tags[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a list comprehension to parse all the `article` tags in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_repositories = [parse_repository(tag) for tag in article_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47500,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'},\n",
       " {'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45200,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'TensorFlow-Examples',\n",
       "  'owner_username': 'aymericdamien',\n",
       "  'stars': 40400,\n",
       "  'repository_url': 'https://github.com/aymericdamien/TensorFlow-Examples'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_repositories[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **QUESTION**: Write a function that takes a `BeautifulSoup` object representing a topic page and returns a list of dictionaries containing information about the top repositories for the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_repositories(doc):\n",
    "    article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')\n",
    "    topic_repos = [parse_repository(tag) for tag in article_tags]\n",
    "    return topic_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the functions we've defined to get the top repositories for any topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47500,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'},\n",
       " {'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45200,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'TensorFlow-Examples',\n",
       "  'owner_username': 'aymericdamien',\n",
       "  'stars': 40400,\n",
       "  'repository_url': 'https://github.com/aymericdamien/TensorFlow-Examples'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_ml = get_topic_page('machine-learning')\n",
    "top_repos_ml = get_top_repositories(topic_page_ml)\n",
    "top_repos_ml[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top repositories for the keyword `data-analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45200,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'superset',\n",
       "  'owner_username': 'apache',\n",
       "  'stars': 37500,\n",
       "  'repository_url': 'https://github.com/apache/superset'},\n",
       " {'repository_name': 'pandas',\n",
       "  'owner_username': 'pandas-dev',\n",
       "  'stars': 29300,\n",
       "  'repository_url': 'https://github.com/pandas-dev/pandas'},\n",
       " {'repository_name': 'metabase',\n",
       "  'owner_username': 'metabase',\n",
       "  'stars': 24500,\n",
       "  'repository_url': 'https://github.com/metabase/metabase'},\n",
       " {'repository_name': 'streamlit',\n",
       "  'owner_username': 'streamlit',\n",
       "  'stars': 14100,\n",
       "  'repository_url': 'https://github.com/streamlit/streamlit'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_da = get_topic_page('data-analysis')\n",
    "top_repos_da = get_top_repositories(topic_page_da)\n",
    "top_repos_da[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top repositories for the keyword `python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'system-design-primer',\n",
       "  'owner_username': 'donnemartin',\n",
       "  'stars': 126000,\n",
       "  'repository_url': 'https://github.com/donnemartin/system-design-primer'},\n",
       " {'repository_name': 'CS-Notes',\n",
       "  'owner_username': 'CyC2018',\n",
       "  'stars': 126000,\n",
       "  'repository_url': 'https://github.com/CyC2018/CS-Notes'},\n",
       " {'repository_name': 'Python',\n",
       "  'owner_username': 'TheAlgorithms',\n",
       "  'stars': 102000,\n",
       "  'repository_url': 'https://github.com/TheAlgorithms/Python'},\n",
       " {'repository_name': 'awesome-python',\n",
       "  'owner_username': 'vinta',\n",
       "  'stars': 95800,\n",
       "  'repository_url': 'https://github.com/vinta/awesome-python'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_repositories(get_topic_page('python'))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the power of defining functions and using libraries? With just one line of code, we can scrape GitHub and find the top repositories for any topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing information to CSV files\n",
    "\n",
    "Let's create a helper function which takes a list of dictionaries and writes them to a CSV file.\n",
    "\n",
    "The input to our function will be a list of dictionary of the form:\n",
    "\n",
    "```\n",
    "[\n",
    "  {'key1': 'abc', 'key2': 'def', 'key3': 'ghi'},\n",
    "  {'key1': 'jkl', 'key2': 'mno', 'key3': 'pqr'},\n",
    "  {'key1': 'stu', 'key2': 'vwx', 'key3': 'yza'}\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "The function will create a file with a given name containing the following data:\n",
    "\n",
    "```\n",
    "key1,key2,key3\n",
    "abc,def,ghi\n",
    "jkl,mno,pqr\n",
    "stu,vwx,yza\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(items, path):\n",
    "    # Open the file in write mode\n",
    "    with open(path, 'w') as f:\n",
    "        # Return if there's nothing to write\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        \n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the data stored in `top_repos_ml` into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_repos_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47500,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_repos_ml[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(top_repositories, 'machine-learning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read the file and inspect its contents. The contents of the file can also be inspected using the \"File > Open\" menu option within Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository_name,owner_username,stars,repository_url\n",
      "tensorflow,tensorflow,155000,https://github.com/tensorflow/tensorflow\n",
      "keras,keras-team,51000,https://github.com/keras-team/keras\n",
      "pytorch,pytorch,47500,https://github.com/pytorch/pytorch\n",
      "scikit-learn,scikit-learn,45200,https://github.com/scikit-learn/scikit-learn\n",
      "TensorFlow-Examples,aymericdamien,40400,https://github.com/aymericdamien/TensorFlow-Examples\n",
      "tesseract,tesseract-ocr,39500,https://github.com/tesseract-ocr/tesseract\n",
      "face_recognition,ageitgey,39400,https://github.com/ageitgey/face_recognition\n",
      "faceswap,deepfakes,34900,https://github.com/deepfakes/faceswap\n",
      "julia,JuliaLang,33300,https://github.com/JuliaLang/julia\n",
      "100-Days-Of-ML-Code,Avik-Jain,31900,https://github.com/Avik-Jain/100-Days-Of-ML-Code\n",
      "awesome-scalability,binhnguyennus,31600,https://github.com/binhnguyennus/awesome-scalability\n",
      "caffe,BVLC,31600,https://github.com/BVLC/caffe\n",
      "madewithml,GokuMohandas,25700,https://github.com/GokuMohandas/madewithml\n",
      "machine-learning-for-software-engineers,ZuzooVn,24900,https://github.com/ZuzooVn/machine-learning-for-software-engineers\n",
      "awesome-deep-learning-papers,terryum,22800,https://github.com/terryum/awesome-deep-learning-papers\n",
      "handson-ml,ageron,22700,https://github.com/ageron/handson-ml\n",
      "d2l-zh,d2l-ai,22300,https://github.com/d2l-ai/d2l-zh\n",
      "cs-video-courses,Developer-Y,22200,https://github.com/Developer-Y/cs-video-courses\n",
      "Coursera-ML-AndrewNg-Notes,fengdu78,21600,https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes\n",
      "data-science-ipython-notebooks,donnemartin,20900,https://github.com/donnemartin/data-science-ipython-notebooks\n",
      "xgboost,dmlc,20800,https://github.com/dmlc/xgboost\n",
      "fastai,fastai,20700,https://github.com/fastai/fastai\n",
      "openpose,CMU-Perceptual-Computing-Lab,20600,https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
      "spaCy,explosion,20100,https://github.com/explosion/spaCy\n",
      "ML-From-Scratch,eriklindernoren,19700,https://github.com/eriklindernoren/ML-From-Scratch\n",
      "NLP-progress,sebastianruder,18300,https://github.com/sebastianruder/NLP-progress\n",
      "homemade-machine-learning,trekhleb,17400,https://github.com/trekhleb/homemade-machine-learning\n",
      "DeepSpeech,mozilla,17000,https://github.com/mozilla/DeepSpeech\n",
      "CNTK,microsoft,17000,https://github.com/microsoft/CNTK\n",
      "awesome-deep-learning,ChristosChristofidis,16900,https://github.com/ChristosChristofidis/awesome-deep-learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('machine-learning.csv', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We've created a CSV containing the information about the top GitHub repositories for the topic `machine-learning`. We can now put together everything we've done so far to solve the original problem.\n",
    "\n",
    "> **QUESTION**: Write a Python function that creates a CSV file (comma-separated values) containing details about the 25 top GitHub repositories for any given topic. The top repositories for the topic `machine-learning` can be found on this page: [https://github.com/topics/machine-learning](https://github.com/topics/machine-learning). The output CSV should contain these details: repository name, owner's username, no. of stars, repository URL. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "base_url = 'https://gitub.com'\n",
    "\n",
    "def scrape_topic_repositories(topic, path=None):\n",
    "    \"\"\"Get the top repositories for a topic and write them to a CSV file\"\"\"\n",
    "    if path is None:\n",
    "        path = topic + '.csv'\n",
    "    topic_page_doc = get_topic_page(topic)\n",
    "    topic_repositories = get_top_repositories(topic_page_doc)\n",
    "    write_csv(topic_repositories, path)\n",
    "    print('Top repositories for topic \"{}\" written to file \"{}\"'.format(topic, path))\n",
    "    return path\n",
    "\n",
    "def get_top_repositories(doc):\n",
    "    \"\"\"Parse the top repositories for a topic given a Beautiful Soup document\"\"\"\n",
    "    article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')\n",
    "    topic_repos = [parse_repository(tag) for tag in article_tags]\n",
    "    return topic_repos\n",
    "\n",
    "def get_topic_page(topic):\n",
    "    \"\"\"Get the web page containing the top repositories for a topic as a Beautiful Soup document\"\"\"\n",
    "    topic_repos_url = 'https://github.com/topics/' + topic\n",
    "    response = requests.get(topic_repos_url)\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + topic_repos_url)\n",
    "    return BeautifulSoup(response.text)    \n",
    "\n",
    "def parse_repository(article_tag):\n",
    "    \"\"\"Parse information about a repository from an <article> tag\"\"\"\n",
    "    a_tags = article_tag.h1.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href'].strip()\n",
    "    stars_tag = article_tag.find('a', class_='social-count float-none')\n",
    "    star_count = parse_star_count(stars_tag.text.strip())\n",
    "    return {'repository_name': repo_name, 'owner_username': username, 'stars': star_count, 'repository_url': repo_url}\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    \"\"\"Parse strings like 40.3k and get the no. of stars as a number\"\"\"\n",
    "    stars_str = stars_str.strip()\n",
    "    return int(float(stars_str[:-1]) * 1000) if stars_str[-1] == 'k' else int(stars_str)\n",
    "\n",
    "def write_csv(items, path):\n",
    "    \"\"\"Write a list of dictionaries to a CSV file\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code of this problem is only about 50 lines long. Isn't that neat? \n",
    "\n",
    "Put another way, if you understand these 50 lines of code, you know pretty much all there is to know about web scraping. Use the interactive nature of Jupyter to experiment with each function and add print statements wherever required to display intermediate output. Reading and understanding code is an essential skill for programmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"machine-learning\" written to file \"machine-learning.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'machine-learning.csv'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('machine-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a CSV file, we can use the `pandas` library to view its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>155000</td>\n",
       "      <td>https://gitub.com/tensorflow/tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keras</td>\n",
       "      <td>keras-team</td>\n",
       "      <td>51000</td>\n",
       "      <td>https://gitub.com/keras-team/keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>47500</td>\n",
       "      <td>https://gitub.com/pytorch/pytorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45200</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TensorFlow-Examples</td>\n",
       "      <td>aymericdamien</td>\n",
       "      <td>40400</td>\n",
       "      <td>https://gitub.com/aymericdamien/TensorFlow-Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tesseract</td>\n",
       "      <td>tesseract-ocr</td>\n",
       "      <td>39500</td>\n",
       "      <td>https://gitub.com/tesseract-ocr/tesseract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>face_recognition</td>\n",
       "      <td>ageitgey</td>\n",
       "      <td>39400</td>\n",
       "      <td>https://gitub.com/ageitgey/face_recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faceswap</td>\n",
       "      <td>deepfakes</td>\n",
       "      <td>34900</td>\n",
       "      <td>https://gitub.com/deepfakes/faceswap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>julia</td>\n",
       "      <td>JuliaLang</td>\n",
       "      <td>33300</td>\n",
       "      <td>https://gitub.com/JuliaLang/julia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100-Days-Of-ML-Code</td>\n",
       "      <td>Avik-Jain</td>\n",
       "      <td>31900</td>\n",
       "      <td>https://gitub.com/Avik-Jain/100-Days-Of-ML-Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>awesome-scalability</td>\n",
       "      <td>binhnguyennus</td>\n",
       "      <td>31600</td>\n",
       "      <td>https://gitub.com/binhnguyennus/awesome-scalab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>caffe</td>\n",
       "      <td>BVLC</td>\n",
       "      <td>31600</td>\n",
       "      <td>https://gitub.com/BVLC/caffe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>madewithml</td>\n",
       "      <td>GokuMohandas</td>\n",
       "      <td>25700</td>\n",
       "      <td>https://gitub.com/GokuMohandas/madewithml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>machine-learning-for-software-engineers</td>\n",
       "      <td>ZuzooVn</td>\n",
       "      <td>24900</td>\n",
       "      <td>https://gitub.com/ZuzooVn/machine-learning-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>awesome-deep-learning-papers</td>\n",
       "      <td>terryum</td>\n",
       "      <td>22800</td>\n",
       "      <td>https://gitub.com/terryum/awesome-deep-learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>handson-ml</td>\n",
       "      <td>ageron</td>\n",
       "      <td>22700</td>\n",
       "      <td>https://gitub.com/ageron/handson-ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d2l-zh</td>\n",
       "      <td>d2l-ai</td>\n",
       "      <td>22300</td>\n",
       "      <td>https://gitub.com/d2l-ai/d2l-zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cs-video-courses</td>\n",
       "      <td>Developer-Y</td>\n",
       "      <td>22200</td>\n",
       "      <td>https://gitub.com/Developer-Y/cs-video-courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Coursera-ML-AndrewNg-Notes</td>\n",
       "      <td>fengdu78</td>\n",
       "      <td>21600</td>\n",
       "      <td>https://gitub.com/fengdu78/Coursera-ML-AndrewN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data-science-ipython-notebooks</td>\n",
       "      <td>donnemartin</td>\n",
       "      <td>20900</td>\n",
       "      <td>https://gitub.com/donnemartin/data-science-ipy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>dmlc</td>\n",
       "      <td>20800</td>\n",
       "      <td>https://gitub.com/dmlc/xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fastai</td>\n",
       "      <td>fastai</td>\n",
       "      <td>20700</td>\n",
       "      <td>https://gitub.com/fastai/fastai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>openpose</td>\n",
       "      <td>CMU-Perceptual-Computing-Lab</td>\n",
       "      <td>20600</td>\n",
       "      <td>https://gitub.com/CMU-Perceptual-Computing-Lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spaCy</td>\n",
       "      <td>explosion</td>\n",
       "      <td>20100</td>\n",
       "      <td>https://gitub.com/explosion/spaCy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ML-From-Scratch</td>\n",
       "      <td>eriklindernoren</td>\n",
       "      <td>19700</td>\n",
       "      <td>https://gitub.com/eriklindernoren/ML-From-Scratch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NLP-progress</td>\n",
       "      <td>sebastianruder</td>\n",
       "      <td>18300</td>\n",
       "      <td>https://gitub.com/sebastianruder/NLP-progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>homemade-machine-learning</td>\n",
       "      <td>trekhleb</td>\n",
       "      <td>17400</td>\n",
       "      <td>https://gitub.com/trekhleb/homemade-machine-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DeepSpeech</td>\n",
       "      <td>mozilla</td>\n",
       "      <td>17000</td>\n",
       "      <td>https://gitub.com/mozilla/DeepSpeech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CNTK</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>17000</td>\n",
       "      <td>https://gitub.com/microsoft/CNTK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>awesome-deep-learning</td>\n",
       "      <td>ChristosChristofidis</td>\n",
       "      <td>16900</td>\n",
       "      <td>https://gitub.com/ChristosChristofidis/awesome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            repository_name                owner_username  \\\n",
       "0                                tensorflow                    tensorflow   \n",
       "1                                     keras                    keras-team   \n",
       "2                                   pytorch                       pytorch   \n",
       "3                              scikit-learn                  scikit-learn   \n",
       "4                       TensorFlow-Examples                 aymericdamien   \n",
       "5                                 tesseract                 tesseract-ocr   \n",
       "6                          face_recognition                      ageitgey   \n",
       "7                                  faceswap                     deepfakes   \n",
       "8                                     julia                     JuliaLang   \n",
       "9                       100-Days-Of-ML-Code                     Avik-Jain   \n",
       "10                      awesome-scalability                 binhnguyennus   \n",
       "11                                    caffe                          BVLC   \n",
       "12                               madewithml                  GokuMohandas   \n",
       "13  machine-learning-for-software-engineers                       ZuzooVn   \n",
       "14             awesome-deep-learning-papers                       terryum   \n",
       "15                               handson-ml                        ageron   \n",
       "16                                   d2l-zh                        d2l-ai   \n",
       "17                         cs-video-courses                   Developer-Y   \n",
       "18               Coursera-ML-AndrewNg-Notes                      fengdu78   \n",
       "19           data-science-ipython-notebooks                   donnemartin   \n",
       "20                                  xgboost                          dmlc   \n",
       "21                                   fastai                        fastai   \n",
       "22                                 openpose  CMU-Perceptual-Computing-Lab   \n",
       "23                                    spaCy                     explosion   \n",
       "24                          ML-From-Scratch               eriklindernoren   \n",
       "25                             NLP-progress                sebastianruder   \n",
       "26                homemade-machine-learning                      trekhleb   \n",
       "27                               DeepSpeech                       mozilla   \n",
       "28                                     CNTK                     microsoft   \n",
       "29                    awesome-deep-learning          ChristosChristofidis   \n",
       "\n",
       "     stars                                     repository_url  \n",
       "0   155000            https://gitub.com/tensorflow/tensorflow  \n",
       "1    51000                 https://gitub.com/keras-team/keras  \n",
       "2    47500                  https://gitub.com/pytorch/pytorch  \n",
       "3    45200        https://gitub.com/scikit-learn/scikit-learn  \n",
       "4    40400  https://gitub.com/aymericdamien/TensorFlow-Exa...  \n",
       "5    39500          https://gitub.com/tesseract-ocr/tesseract  \n",
       "6    39400        https://gitub.com/ageitgey/face_recognition  \n",
       "7    34900               https://gitub.com/deepfakes/faceswap  \n",
       "8    33300                  https://gitub.com/JuliaLang/julia  \n",
       "9    31900    https://gitub.com/Avik-Jain/100-Days-Of-ML-Code  \n",
       "10   31600  https://gitub.com/binhnguyennus/awesome-scalab...  \n",
       "11   31600                       https://gitub.com/BVLC/caffe  \n",
       "12   25700          https://gitub.com/GokuMohandas/madewithml  \n",
       "13   24900  https://gitub.com/ZuzooVn/machine-learning-for...  \n",
       "14   22800  https://gitub.com/terryum/awesome-deep-learnin...  \n",
       "15   22700                https://gitub.com/ageron/handson-ml  \n",
       "16   22300                    https://gitub.com/d2l-ai/d2l-zh  \n",
       "17   22200     https://gitub.com/Developer-Y/cs-video-courses  \n",
       "18   21600  https://gitub.com/fengdu78/Coursera-ML-AndrewN...  \n",
       "19   20900  https://gitub.com/donnemartin/data-science-ipy...  \n",
       "20   20800                     https://gitub.com/dmlc/xgboost  \n",
       "21   20700                    https://gitub.com/fastai/fastai  \n",
       "22   20600  https://gitub.com/CMU-Perceptual-Computing-Lab...  \n",
       "23   20100                  https://gitub.com/explosion/spaCy  \n",
       "24   19700  https://gitub.com/eriklindernoren/ML-From-Scratch  \n",
       "25   18300      https://gitub.com/sebastianruder/NLP-progress  \n",
       "26   17400  https://gitub.com/trekhleb/homemade-machine-le...  \n",
       "27   17000               https://gitub.com/mozilla/DeepSpeech  \n",
       "28   17000                   https://gitub.com/microsoft/CNTK  \n",
       "29   16900  https://gitub.com/ChristosChristofidis/awesome...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('machine-learning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"data-analysis\" written to file \"data-analysis.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data-analysis.csv'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('data-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45200</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superset</td>\n",
       "      <td>apache</td>\n",
       "      <td>37500</td>\n",
       "      <td>https://gitub.com/apache/superset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pandas</td>\n",
       "      <td>pandas-dev</td>\n",
       "      <td>29300</td>\n",
       "      <td>https://gitub.com/pandas-dev/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metabase</td>\n",
       "      <td>metabase</td>\n",
       "      <td>24500</td>\n",
       "      <td>https://gitub.com/metabase/metabase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>streamlit</td>\n",
       "      <td>streamlit</td>\n",
       "      <td>14100</td>\n",
       "      <td>https://gitub.com/streamlit/streamlit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goaccess</td>\n",
       "      <td>allinurl</td>\n",
       "      <td>13100</td>\n",
       "      <td>https://gitub.com/allinurl/goaccess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CyberChef</td>\n",
       "      <td>gchq</td>\n",
       "      <td>11700</td>\n",
       "      <td>https://gitub.com/gchq/CyberChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI-Expert-Roadmap</td>\n",
       "      <td>AMAI-GmbH</td>\n",
       "      <td>9900</td>\n",
       "      <td>https://gitub.com/AMAI-GmbH/AI-Expert-Roadmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenRefine</td>\n",
       "      <td>OpenRefine</td>\n",
       "      <td>8000</td>\n",
       "      <td>https://gitub.com/OpenRefine/OpenRefine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mlcourse.ai</td>\n",
       "      <td>Yorko</td>\n",
       "      <td>7600</td>\n",
       "      <td>https://gitub.com/Yorko/mlcourse.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pandas-profiling</td>\n",
       "      <td>pandas-profiling</td>\n",
       "      <td>7100</td>\n",
       "      <td>https://gitub.com/pandas-profiling/pandas-prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>statsmodels</td>\n",
       "      <td>statsmodels</td>\n",
       "      <td>6200</td>\n",
       "      <td>https://gitub.com/statsmodels/statsmodels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pandas_exercises</td>\n",
       "      <td>guipsamora</td>\n",
       "      <td>6000</td>\n",
       "      <td>https://gitub.com/guipsamora/pandas_exercises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>imbalanced-learn</td>\n",
       "      <td>scikit-learn-contrib</td>\n",
       "      <td>5200</td>\n",
       "      <td>https://gitub.com/scikit-learn-contrib/imbalan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>alluxio</td>\n",
       "      <td>Alluxio</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/Alluxio/alluxio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data-Analysis-and-Machine-Learning-Projects</td>\n",
       "      <td>rhiever</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/rhiever/Data-Analysis-and-Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pachyderm</td>\n",
       "      <td>pachyderm</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/pachyderm/pachyderm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gonum</td>\n",
       "      <td>gonum</td>\n",
       "      <td>4800</td>\n",
       "      <td>https://gitub.com/gonum/gonum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>knowledge-repo</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>4700</td>\n",
       "      <td>https://gitub.com/airbnb/knowledge-repo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gop</td>\n",
       "      <td>goplus</td>\n",
       "      <td>4600</td>\n",
       "      <td>https://gitub.com/goplus/gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>weibospider</td>\n",
       "      <td>SpiderClub</td>\n",
       "      <td>4600</td>\n",
       "      <td>https://gitub.com/SpiderClub/weibospider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>awesome-R</td>\n",
       "      <td>qinwf</td>\n",
       "      <td>4500</td>\n",
       "      <td>https://gitub.com/qinwf/awesome-R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pyod</td>\n",
       "      <td>yzhao062</td>\n",
       "      <td>4400</td>\n",
       "      <td>https://gitub.com/yzhao062/pyod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pydata-notebook</td>\n",
       "      <td>BrambleXu</td>\n",
       "      <td>4100</td>\n",
       "      <td>https://gitub.com/BrambleXu/pydata-notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sqlpad</td>\n",
       "      <td>sqlpad</td>\n",
       "      <td>3700</td>\n",
       "      <td>https://gitub.com/sqlpad/sqlpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>akshare</td>\n",
       "      <td>jindaxiang</td>\n",
       "      <td>3400</td>\n",
       "      <td>https://gitub.com/jindaxiang/akshare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ai-Learn</td>\n",
       "      <td>tangyudi</td>\n",
       "      <td>3200</td>\n",
       "      <td>https://gitub.com/tangyudi/Ai-Learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>xlearn</td>\n",
       "      <td>aksnzhy</td>\n",
       "      <td>2900</td>\n",
       "      <td>https://gitub.com/aksnzhy/xlearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>missingno</td>\n",
       "      <td>ResidentMario</td>\n",
       "      <td>2700</td>\n",
       "      <td>https://gitub.com/ResidentMario/missingno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>plotnine</td>\n",
       "      <td>has2k1</td>\n",
       "      <td>2700</td>\n",
       "      <td>https://gitub.com/has2k1/plotnine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                repository_name        owner_username  stars  \\\n",
       "0                                  scikit-learn          scikit-learn  45200   \n",
       "1                                      superset                apache  37500   \n",
       "2                                        pandas            pandas-dev  29300   \n",
       "3                                      metabase              metabase  24500   \n",
       "4                                     streamlit             streamlit  14100   \n",
       "5                                      goaccess              allinurl  13100   \n",
       "6                                     CyberChef                  gchq  11700   \n",
       "7                             AI-Expert-Roadmap             AMAI-GmbH   9900   \n",
       "8                                    OpenRefine            OpenRefine   8000   \n",
       "9                                   mlcourse.ai                 Yorko   7600   \n",
       "10                             pandas-profiling      pandas-profiling   7100   \n",
       "11                                  statsmodels           statsmodels   6200   \n",
       "12                             pandas_exercises            guipsamora   6000   \n",
       "13                             imbalanced-learn  scikit-learn-contrib   5200   \n",
       "14                                      alluxio               Alluxio   5000   \n",
       "15  Data-Analysis-and-Machine-Learning-Projects               rhiever   5000   \n",
       "16                                    pachyderm             pachyderm   5000   \n",
       "17                                        gonum                 gonum   4800   \n",
       "18                               knowledge-repo                airbnb   4700   \n",
       "19                                          gop                goplus   4600   \n",
       "20                                  weibospider            SpiderClub   4600   \n",
       "21                                    awesome-R                 qinwf   4500   \n",
       "22                                         pyod              yzhao062   4400   \n",
       "23                              pydata-notebook             BrambleXu   4100   \n",
       "24                                       sqlpad                sqlpad   3700   \n",
       "25                                      akshare            jindaxiang   3400   \n",
       "26                                     Ai-Learn              tangyudi   3200   \n",
       "27                                       xlearn               aksnzhy   2900   \n",
       "28                                    missingno         ResidentMario   2700   \n",
       "29                                     plotnine                has2k1   2700   \n",
       "\n",
       "                                       repository_url  \n",
       "0         https://gitub.com/scikit-learn/scikit-learn  \n",
       "1                   https://gitub.com/apache/superset  \n",
       "2                 https://gitub.com/pandas-dev/pandas  \n",
       "3                 https://gitub.com/metabase/metabase  \n",
       "4               https://gitub.com/streamlit/streamlit  \n",
       "5                 https://gitub.com/allinurl/goaccess  \n",
       "6                    https://gitub.com/gchq/CyberChef  \n",
       "7       https://gitub.com/AMAI-GmbH/AI-Expert-Roadmap  \n",
       "8             https://gitub.com/OpenRefine/OpenRefine  \n",
       "9                 https://gitub.com/Yorko/mlcourse.ai  \n",
       "10  https://gitub.com/pandas-profiling/pandas-prof...  \n",
       "11          https://gitub.com/statsmodels/statsmodels  \n",
       "12      https://gitub.com/guipsamora/pandas_exercises  \n",
       "13  https://gitub.com/scikit-learn-contrib/imbalan...  \n",
       "14                  https://gitub.com/Alluxio/alluxio  \n",
       "15  https://gitub.com/rhiever/Data-Analysis-and-Ma...  \n",
       "16              https://gitub.com/pachyderm/pachyderm  \n",
       "17                      https://gitub.com/gonum/gonum  \n",
       "18            https://gitub.com/airbnb/knowledge-repo  \n",
       "19                       https://gitub.com/goplus/gop  \n",
       "20           https://gitub.com/SpiderClub/weibospider  \n",
       "21                  https://gitub.com/qinwf/awesome-R  \n",
       "22                    https://gitub.com/yzhao062/pyod  \n",
       "23        https://gitub.com/BrambleXu/pydata-notebook  \n",
       "24                    https://gitub.com/sqlpad/sqlpad  \n",
       "25               https://gitub.com/jindaxiang/akshare  \n",
       "26                https://gitub.com/tangyudi/Ai-Learn  \n",
       "27                   https://gitub.com/aksnzhy/xlearn  \n",
       "28          https://gitub.com/ResidentMario/missingno  \n",
       "29                  https://gitub.com/has2k1/plotnine  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data-analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"python\" written to file \"python.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'python.csv'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>155000</td>\n",
       "      <td>https://gitub.com/tensorflow/tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>system-design-primer</td>\n",
       "      <td>donnemartin</td>\n",
       "      <td>126000</td>\n",
       "      <td>https://gitub.com/donnemartin/system-design-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS-Notes</td>\n",
       "      <td>CyC2018</td>\n",
       "      <td>126000</td>\n",
       "      <td>https://gitub.com/CyC2018/CS-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>TheAlgorithms</td>\n",
       "      <td>102000</td>\n",
       "      <td>https://gitub.com/TheAlgorithms/Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome-python</td>\n",
       "      <td>vinta</td>\n",
       "      <td>95800</td>\n",
       "      <td>https://gitub.com/vinta/awesome-python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>free-programming-books-zh_CN</td>\n",
       "      <td>justjavac</td>\n",
       "      <td>78500</td>\n",
       "      <td>https://gitub.com/justjavac/free-programming-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thefuck</td>\n",
       "      <td>nvbn</td>\n",
       "      <td>59800</td>\n",
       "      <td>https://gitub.com/nvbn/thefuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>django</td>\n",
       "      <td>django</td>\n",
       "      <td>56700</td>\n",
       "      <td>https://gitub.com/django/django</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flask</td>\n",
       "      <td>pallets</td>\n",
       "      <td>54500</td>\n",
       "      <td>https://gitub.com/pallets/flask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>keras</td>\n",
       "      <td>keras-team</td>\n",
       "      <td>51000</td>\n",
       "      <td>https://gitub.com/keras-team/keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>httpie</td>\n",
       "      <td>httpie</td>\n",
       "      <td>50400</td>\n",
       "      <td>https://gitub.com/httpie/httpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ansible</td>\n",
       "      <td>ansible</td>\n",
       "      <td>47700</td>\n",
       "      <td>https://gitub.com/ansible/ansible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>47500</td>\n",
       "      <td>https://gitub.com/pytorch/pytorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>project-based-learning</td>\n",
       "      <td>tuvtran</td>\n",
       "      <td>47200</td>\n",
       "      <td>https://gitub.com/tuvtran/project-based-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45200</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>requests</td>\n",
       "      <td>psf</td>\n",
       "      <td>45000</td>\n",
       "      <td>https://gitub.com/psf/requests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>core</td>\n",
       "      <td>home-assistant</td>\n",
       "      <td>41900</td>\n",
       "      <td>https://gitub.com/home-assistant/core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>leetcode</td>\n",
       "      <td>azl397985856</td>\n",
       "      <td>41300</td>\n",
       "      <td>https://gitub.com/azl397985856/leetcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TensorFlow-Examples</td>\n",
       "      <td>aymericdamien</td>\n",
       "      <td>40400</td>\n",
       "      <td>https://gitub.com/aymericdamien/TensorFlow-Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>40300</td>\n",
       "      <td>https://gitub.com/scrapy/scrapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HelloGitHub</td>\n",
       "      <td>521xueweihan</td>\n",
       "      <td>40000</td>\n",
       "      <td>https://gitub.com/521xueweihan/HelloGitHub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>face_recognition</td>\n",
       "      <td>ageitgey</td>\n",
       "      <td>39400</td>\n",
       "      <td>https://gitub.com/ageitgey/face_recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>superset</td>\n",
       "      <td>apache</td>\n",
       "      <td>37500</td>\n",
       "      <td>https://gitub.com/apache/superset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>manim</td>\n",
       "      <td>3b1b</td>\n",
       "      <td>32800</td>\n",
       "      <td>https://gitub.com/3b1b/manim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100-Days-Of-ML-Code</td>\n",
       "      <td>Avik-Jain</td>\n",
       "      <td>31900</td>\n",
       "      <td>https://gitub.com/Avik-Jain/100-Days-Of-ML-Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fastapi</td>\n",
       "      <td>tiangolo</td>\n",
       "      <td>29500</td>\n",
       "      <td>https://gitub.com/tiangolo/fastapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AiLearning</td>\n",
       "      <td>apachecn</td>\n",
       "      <td>29400</td>\n",
       "      <td>https://gitub.com/apachecn/AiLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spark</td>\n",
       "      <td>apache</td>\n",
       "      <td>29300</td>\n",
       "      <td>https://gitub.com/apache/spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pandas</td>\n",
       "      <td>pandas-dev</td>\n",
       "      <td>29300</td>\n",
       "      <td>https://gitub.com/pandas-dev/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PythonDataScienceHandbook</td>\n",
       "      <td>jakevdp</td>\n",
       "      <td>28800</td>\n",
       "      <td>https://gitub.com/jakevdp/PythonDataScienceHan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repository_name  owner_username   stars  \\\n",
       "0                     tensorflow      tensorflow  155000   \n",
       "1           system-design-primer     donnemartin  126000   \n",
       "2                       CS-Notes         CyC2018  126000   \n",
       "3                         Python   TheAlgorithms  102000   \n",
       "4                 awesome-python           vinta   95800   \n",
       "5   free-programming-books-zh_CN       justjavac   78500   \n",
       "6                        thefuck            nvbn   59800   \n",
       "7                         django          django   56700   \n",
       "8                          flask         pallets   54500   \n",
       "9                          keras      keras-team   51000   \n",
       "10                        httpie          httpie   50400   \n",
       "11                       ansible         ansible   47700   \n",
       "12                       pytorch         pytorch   47500   \n",
       "13        project-based-learning         tuvtran   47200   \n",
       "14                  scikit-learn    scikit-learn   45200   \n",
       "15                      requests             psf   45000   \n",
       "16                          core  home-assistant   41900   \n",
       "17                      leetcode    azl397985856   41300   \n",
       "18           TensorFlow-Examples   aymericdamien   40400   \n",
       "19                        scrapy          scrapy   40300   \n",
       "20                   HelloGitHub    521xueweihan   40000   \n",
       "21              face_recognition        ageitgey   39400   \n",
       "22                      superset          apache   37500   \n",
       "23                         manim            3b1b   32800   \n",
       "24           100-Days-Of-ML-Code       Avik-Jain   31900   \n",
       "25                       fastapi        tiangolo   29500   \n",
       "26                    AiLearning        apachecn   29400   \n",
       "27                         spark          apache   29300   \n",
       "28                        pandas      pandas-dev   29300   \n",
       "29     PythonDataScienceHandbook         jakevdp   28800   \n",
       "\n",
       "                                       repository_url  \n",
       "0             https://gitub.com/tensorflow/tensorflow  \n",
       "1   https://gitub.com/donnemartin/system-design-pr...  \n",
       "2                  https://gitub.com/CyC2018/CS-Notes  \n",
       "3              https://gitub.com/TheAlgorithms/Python  \n",
       "4              https://gitub.com/vinta/awesome-python  \n",
       "5   https://gitub.com/justjavac/free-programming-b...  \n",
       "6                      https://gitub.com/nvbn/thefuck  \n",
       "7                     https://gitub.com/django/django  \n",
       "8                     https://gitub.com/pallets/flask  \n",
       "9                  https://gitub.com/keras-team/keras  \n",
       "10                    https://gitub.com/httpie/httpie  \n",
       "11                  https://gitub.com/ansible/ansible  \n",
       "12                  https://gitub.com/pytorch/pytorch  \n",
       "13   https://gitub.com/tuvtran/project-based-learning  \n",
       "14        https://gitub.com/scikit-learn/scikit-learn  \n",
       "15                     https://gitub.com/psf/requests  \n",
       "16              https://gitub.com/home-assistant/core  \n",
       "17            https://gitub.com/azl397985856/leetcode  \n",
       "18  https://gitub.com/aymericdamien/TensorFlow-Exa...  \n",
       "19                    https://gitub.com/scrapy/scrapy  \n",
       "20         https://gitub.com/521xueweihan/HelloGitHub  \n",
       "21        https://gitub.com/ageitgey/face_recognition  \n",
       "22                  https://gitub.com/apache/superset  \n",
       "23                       https://gitub.com/3b1b/manim  \n",
       "24    https://gitub.com/Avik-Jain/100-Days-Of-ML-Code  \n",
       "25                 https://gitub.com/tiangolo/fastapi  \n",
       "26              https://gitub.com/apachecn/AiLearning  \n",
       "27                     https://gitub.com/apache/spark  \n",
       "28                https://gitub.com/pandas-dev/pandas  \n",
       "29  https://gitub.com/jakevdp/PythonDataScienceHan...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('python.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can go even further and write a function that scrapes top repositories for several topics.\n",
    "\n",
    "> **EXERCISE**: Write a function `scrape_topics` which takes a list of topics and creates CSV files containing top repositories for a list of topics. Test it out using the empty cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(files=['machine-learning.csv', 'python.csv', 'data-analysis.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a REST API to retrieve data as JSON\n",
    "\n",
    "Not all URLs point to an HTML page. Consider this URL for example: https://api.github.com/repos/octocat/hello-world . It points to a JSON document, which has a structure like this:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Hello-World\",\n",
    "  \"full_name\": \"octocat/Hello-World\",\n",
    "  \"private\": false,\n",
    "  \"owner\": {\n",
    "    \"login\": \"octocat\",\n",
    "    \"id\": 583231,\n",
    "  },\n",
    "  \"html_url\": \"https://github.com/octocat/Hello-World\",\n",
    "}\n",
    "```\n",
    "\n",
    "It's quite similar to a Python dictionary. In fact, you can use the `json` module from python to convert a JSON document into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.github.com/repos/octocat/hello-world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dict = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1296269,\n",
       " 'node_id': 'MDEwOlJlcG9zaXRvcnkxMjk2MjY5',\n",
       " 'name': 'Hello-World',\n",
       " 'full_name': 'octocat/Hello-World',\n",
       " 'private': False,\n",
       " 'owner': {'login': 'octocat',\n",
       "  'id': 583231,\n",
       "  'node_id': 'MDQ6VXNlcjU4MzIzMQ==',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/583231?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/octocat',\n",
       "  'html_url': 'https://github.com/octocat',\n",
       "  'followers_url': 'https://api.github.com/users/octocat/followers',\n",
       "  'following_url': 'https://api.github.com/users/octocat/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/octocat/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/octocat/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/octocat/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/octocat/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/octocat/repos',\n",
       "  'events_url': 'https://api.github.com/users/octocat/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/octocat/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'html_url': 'https://github.com/octocat/Hello-World',\n",
       " 'description': 'My first repository on GitHub!',\n",
       " 'fork': False,\n",
       " 'url': 'https://api.github.com/repos/octocat/Hello-World',\n",
       " 'forks_url': 'https://api.github.com/repos/octocat/Hello-World/forks',\n",
       " 'keys_url': 'https://api.github.com/repos/octocat/Hello-World/keys{/key_id}',\n",
       " 'collaborators_url': 'https://api.github.com/repos/octocat/Hello-World/collaborators{/collaborator}',\n",
       " 'teams_url': 'https://api.github.com/repos/octocat/Hello-World/teams',\n",
       " 'hooks_url': 'https://api.github.com/repos/octocat/Hello-World/hooks',\n",
       " 'issue_events_url': 'https://api.github.com/repos/octocat/Hello-World/issues/events{/number}',\n",
       " 'events_url': 'https://api.github.com/repos/octocat/Hello-World/events',\n",
       " 'assignees_url': 'https://api.github.com/repos/octocat/Hello-World/assignees{/user}',\n",
       " 'branches_url': 'https://api.github.com/repos/octocat/Hello-World/branches{/branch}',\n",
       " 'tags_url': 'https://api.github.com/repos/octocat/Hello-World/tags',\n",
       " 'blobs_url': 'https://api.github.com/repos/octocat/Hello-World/git/blobs{/sha}',\n",
       " 'git_tags_url': 'https://api.github.com/repos/octocat/Hello-World/git/tags{/sha}',\n",
       " 'git_refs_url': 'https://api.github.com/repos/octocat/Hello-World/git/refs{/sha}',\n",
       " 'trees_url': 'https://api.github.com/repos/octocat/Hello-World/git/trees{/sha}',\n",
       " 'statuses_url': 'https://api.github.com/repos/octocat/Hello-World/statuses/{sha}',\n",
       " 'languages_url': 'https://api.github.com/repos/octocat/Hello-World/languages',\n",
       " 'stargazers_url': 'https://api.github.com/repos/octocat/Hello-World/stargazers',\n",
       " 'contributors_url': 'https://api.github.com/repos/octocat/Hello-World/contributors',\n",
       " 'subscribers_url': 'https://api.github.com/repos/octocat/Hello-World/subscribers',\n",
       " 'subscription_url': 'https://api.github.com/repos/octocat/Hello-World/subscription',\n",
       " 'commits_url': 'https://api.github.com/repos/octocat/Hello-World/commits{/sha}',\n",
       " 'git_commits_url': 'https://api.github.com/repos/octocat/Hello-World/git/commits{/sha}',\n",
       " 'comments_url': 'https://api.github.com/repos/octocat/Hello-World/comments{/number}',\n",
       " 'issue_comment_url': 'https://api.github.com/repos/octocat/Hello-World/issues/comments{/number}',\n",
       " 'contents_url': 'https://api.github.com/repos/octocat/Hello-World/contents/{+path}',\n",
       " 'compare_url': 'https://api.github.com/repos/octocat/Hello-World/compare/{base}...{head}',\n",
       " 'merges_url': 'https://api.github.com/repos/octocat/Hello-World/merges',\n",
       " 'archive_url': 'https://api.github.com/repos/octocat/Hello-World/{archive_format}{/ref}',\n",
       " 'downloads_url': 'https://api.github.com/repos/octocat/Hello-World/downloads',\n",
       " 'issues_url': 'https://api.github.com/repos/octocat/Hello-World/issues{/number}',\n",
       " 'pulls_url': 'https://api.github.com/repos/octocat/Hello-World/pulls{/number}',\n",
       " 'milestones_url': 'https://api.github.com/repos/octocat/Hello-World/milestones{/number}',\n",
       " 'notifications_url': 'https://api.github.com/repos/octocat/Hello-World/notifications{?since,all,participating}',\n",
       " 'labels_url': 'https://api.github.com/repos/octocat/Hello-World/labels{/name}',\n",
       " 'releases_url': 'https://api.github.com/repos/octocat/Hello-World/releases{/id}',\n",
       " 'deployments_url': 'https://api.github.com/repos/octocat/Hello-World/deployments',\n",
       " 'created_at': '2011-01-26T19:01:12Z',\n",
       " 'updated_at': '2021-04-07T00:47:23Z',\n",
       " 'pushed_at': '2021-04-11T10:31:18Z',\n",
       " 'git_url': 'git://github.com/octocat/Hello-World.git',\n",
       " 'ssh_url': 'git@github.com:octocat/Hello-World.git',\n",
       " 'clone_url': 'https://github.com/octocat/Hello-World.git',\n",
       " 'svn_url': 'https://github.com/octocat/Hello-World',\n",
       " 'homepage': '',\n",
       " 'size': 1,\n",
       " 'stargazers_count': 1636,\n",
       " 'watchers_count': 1636,\n",
       " 'language': None,\n",
       " 'has_issues': True,\n",
       " 'has_projects': True,\n",
       " 'has_downloads': True,\n",
       " 'has_wiki': True,\n",
       " 'has_pages': False,\n",
       " 'forks_count': 1538,\n",
       " 'mirror_url': None,\n",
       " 'archived': False,\n",
       " 'disabled': False,\n",
       " 'open_issues_count': 481,\n",
       " 'license': None,\n",
       " 'forks': 1538,\n",
       " 'open_issues': 481,\n",
       " 'watchers': 1636,\n",
       " 'default_branch': 'master',\n",
       " 'temp_clone_token': None,\n",
       " 'network_count': 1538,\n",
       " 'subscribers_count': 1718}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike HTML, it's really easy to work with JSON using Python, simply fetch the contents of the URL and convert it to a dictionary. Such URLs are often called **REST APIs** or REST API endpoints. Many websites offer well-documented REST APIs to access data from the site in JSON format:\n",
    "\n",
    "* GitHub: https://docs.github.com/en/rest/reference/repos\n",
    "* Facebook: https://developers.facebook.com/docs/groups-api/reference\n",
    "* Twitter: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline\n",
    "* Reddit: https://www.reddit.com/dev/api/\n",
    "\n",
    "Using an API is the *officially supported* way of extracting information from a website. To use an API, you will often need to register as a developer on the platform and generate an API key, which you'll need to send with every request to authenticate yourself. \n",
    "\n",
    "Since GitHub offers a public API, we can use it without any restrictions to fetch information about public repositories.\n",
    "\n",
    "\n",
    "> **QUESTION**: Write a function `get_repo_details` to find the following information about a repository: description, watcher count, fork count, open issues count, created at time and updated at time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_details(username, repo_name):\n",
    "    print('Fetching information for {}/{}'.format(username, repo_name))\n",
    "    repo_details_url = \"https://api.github.com/repos/\" + username + \"/\" + repo_name\n",
    "    response = requests.get(repo_details_url)\n",
    "    if not response.ok:\n",
    "        print(\"Failed to fetch!\")\n",
    "        return {}\n",
    "    repo_data = json.loads(response.text)\n",
    "    return {\n",
    "        'description': repo_data['description'],\n",
    "        'watchers': repo_data['watchers_count'],\n",
    "        'open_issues': repo_data['open_issues_count'],\n",
    "        'created_at': repo_data['created_at'],\n",
    "        'updated_at': repo_data['updated_at']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching information for octocat/hello-world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'My first repository on GitHub!',\n",
       " 'watchers': 1636,\n",
       " 'open_issues': 481,\n",
       " 'created_at': '2011-01-26T19:01:12Z',\n",
       " 'updated_at': '2021-04-07T00:47:23Z'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_details('octocat', 'hello-world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching information for tensorflow/tensorflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'An Open Source Machine Learning Framework for Everyone',\n",
       " 'watchers': 154790,\n",
       " 'open_issues': 4050,\n",
       " 'created_at': '2015-11-07T01:19:20Z',\n",
       " 'updated_at': '2021-04-12T14:04:27Z'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_details('tensorflow', 'tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **QUESTION**: Augment the list of top repositories for a topic with the repository description, watcher count, fork count, open issues count, created at time and updated at time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_details(repos):\n",
    "    return [dict(**get_repo_details(repo['owner_username'], repo['repository_name']), **repo) for repo in repos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching information for tensorflow/tensorflow\n",
      "Fetching information for keras-team/keras\n",
      "Fetching information for pytorch/pytorch\n",
      "Fetching information for scikit-learn/scikit-learn\n",
      "Fetching information for aymericdamien/TensorFlow-Examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'description': 'An Open Source Machine Learning Framework for Everyone',\n",
       "  'watchers': 154790,\n",
       "  'open_issues': 4050,\n",
       "  'created_at': '2015-11-07T01:19:20Z',\n",
       "  'updated_at': '2021-04-12T14:04:27Z',\n",
       "  'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'description': 'Deep Learning for humans',\n",
       "  'watchers': 51022,\n",
       "  'open_issues': 3249,\n",
       "  'created_at': '2015-03-28T00:35:42Z',\n",
       "  'updated_at': '2021-04-12T12:22:45Z',\n",
       "  'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'description': 'Tensors and Dynamic neural networks in Python with strong GPU acceleration',\n",
       "  'watchers': 47505,\n",
       "  'open_issues': 8620,\n",
       "  'created_at': '2016-08-13T05:26:41Z',\n",
       "  'updated_at': '2021-04-12T13:32:55Z',\n",
       "  'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47500,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'},\n",
       " {'description': 'scikit-learn: machine learning in Python',\n",
       "  'watchers': 45210,\n",
       "  'open_issues': 2348,\n",
       "  'created_at': '2010-08-17T09:43:38Z',\n",
       "  'updated_at': '2021-04-12T14:06:55Z',\n",
       "  'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45200,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'description': 'TensorFlow Tutorial and Examples for Beginners (support TF v1 & v2)',\n",
       "  'watchers': 40414,\n",
       "  'open_issues': 210,\n",
       "  'created_at': '2015-11-11T14:21:19Z',\n",
       "  'updated_at': '2021-04-12T11:35:34Z',\n",
       "  'repository_name': 'TensorFlow-Examples',\n",
       "  'owner_username': 'aymericdamien',\n",
       "  'stars': 40400,\n",
       "  'repository_url': 'https://github.com/aymericdamien/TensorFlow-Examples'}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_repo_details(top_repositories[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get rate limited if you attempt to make more than 60 requests per hour. To overcome the rate limit, use the Github OAuth token as described here: https://towardsdatascience.com/all-the-things-you-can-do-with-github-api-and-python-f01790fca131\n",
    "\n",
    "Note: Never publish your Github API token publicly, as it can be used to access your Github account. To store your API token without displaying it on the screen, use `getpass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Augment the list of top repositories for a topic with some additional information about the user/organization the repository belong to: name, description, Github URL, no. of repositories, type (user or organization) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronyms\n",
    "\n",
    "In case you're feeling overwhelmed by all the acronyms, here are their expansions:\n",
    "- **REST**: Represetational State Transfer\n",
    "- **API**: Application Programming Interface\n",
    "- **JSON**: JavaScript Object Notation\n",
    "- **URL**: Universal Resource Locator\n",
    "\n",
    "Don't worry, you needn't remember any of them!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Websites by Parsing Links on a Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you scrape you a web page, you are likely to find several links on the page. For, example, on the page https://github.com/topics, you will find links to several topic pages. You can parse all the topic page links from this page, and scrape those pages to get the top repositories for each topic. Further, you can parse all the repository links from a topic page and scrape individual repository pages, and so on. \n",
    "\n",
    "The process of scraping a page, parsing links and then using the links to parsing other pages on the same site is called **web crawling**. It's how search engines like Google are able to index and search data from millions of websites on the internet. Python offer libraries like [Scrapy](https://scrapy.org) for crawling websites easily.\n",
    "\n",
    "You can do some basic crawling with `requests`, Beautiful soup, and few simple `for` loops in Python. Here's an exercise to get you started\n",
    "\n",
    "\n",
    "> **EXERCISE**: Get the top 100 repositories for the all the featured topics on GitHub. You might find these URLs useful:\n",
    "> \n",
    "> * Eighth page of featured topics: https://github.com/topics/?page=8  \n",
    "> * Second page of top repositories for a topic: https://github.com/topics/machine-learning?page=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Further Reading\n",
    "\n",
    "We've covered the following topics in this tutorial:\n",
    "\n",
    "* Downloading web pages using the requests library\n",
    "* Inspecting the HTML source code of a web page\n",
    "* Parsing parts of a website using Beautiful Soup\n",
    "* Writing parsed information into CSV files\n",
    "* Using a REST API to retrieve data as JSON\n",
    "* Combining data from multiple sources\n",
    "* Using links on a page to crawl a website\n",
    "\n",
    "\n",
    "Here are some things to keep in mind w.r.t. web scraping:\n",
    "\n",
    "* Most websites disallow web scraping for commercial purposes\n",
    "* Prefer using web scraping only for learning and research purposes\n",
    "* Some websites may block your IP or stop sending valid information if you send too many requests\n",
    "* Review the terms and conditions of a website before scraping data from it\n",
    "* Remove sensitive and personally identifiable information before publishing a dataset online\n",
    "* Use official REST APIs wherever available, with proper API keys\n",
    "* Scraping data that you see after logging in is harder (it requires special cookies and headers)\n",
    "* Websites change their HTML layout frequently, which may cause your scarping scripts to break\n",
    "\n",
    "\n",
    "Here are some more examples of scraping:\n",
    "\n",
    "* https://medium.com/@msalmon00/web-scraping-job-postings-from-indeed-96bd588dcb4b\n",
    "* https://medium.com/the-innovation/scraping-medium-with-python-beautiful-soup-3314f898bbf5\n",
    "* https://medium.com/brainstation23/how-to-become-a-pro-with-scraping-youtube-videos-in-3-minutes-a6ac56021961\n",
    "* https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\n",
    "* https://www.freecodecamp.org/news/scraping-wikipedia-articles-with-python/\n",
    "* https://towardsdatascience.com/web-scraping-yahoo-finance-477fe3daa852\n",
    "\n",
    "### Project Ideas\n",
    "\n",
    "Here are some project ideas if you're looking to work on a web scraping project. You can work of one of these ideas, or pick something entirely different.\n",
    "\n",
    "\n",
    "1. **Filmography of Actors/Directors (Wikipedia)**: The list of Films and TV shows an actor has been a part of is called their filmography. Here's an example filmography page on Wikipedia: https://en.wikipedia.org/wiki/Christian_Bale_filmography . Can you scrape this information and create a dataset of [filmographies](https://en.wikipedia.org/wiki/Category:American_filmographies) of famous actors/actresses/directors with information like film title, year of release, etc.? \n",
    "\n",
    "\n",
    "2. **Discography of an Artist (Wikipedia)**: The list of albums released by an artist is called their discography. Here's an example discography page on Wikipedia: https://en.wikipedia.org/wiki/Linkin_Park_discography . Can you scrape this information and create a dataset of [discographies](https://en.wikipedia.org/wiki/Linkin_Park_discography) or music albums with information like the album title, release date etc.?\n",
    "\n",
    "\n",
    "3. **Dataset of Movies (TMDb)**: The Movie Database (TMDb) contains information about thousands of movies from around the world: https://www.themoviedb.org/movie . Can you scape the site to create a dataset of movies containing information like title, release date, cast, etc. ? You can also create datasets of movie actors/actresses/directors using this site.\n",
    "\n",
    "\n",
    "4. **Dataset of TV Shows (TMDb)**: The Movie Database (TMDb) contains information about thousands of TV shows from around the world: https://www.themoviedb.org/tv . Can you scape the site to create a dataset of TV shows containing information like title, release date, cast, crew, etc. ? You can also create datasets of TV actors/actresses/directors using this site.\n",
    "\n",
    "\n",
    "5. **Collections of Popular Repositories (GitHub)**: Scape GitHub collections ( https://github.com/collections ) to create a dataset of popular repositories organized by different use cases.\n",
    "\n",
    "\n",
    "6. **Dataset of Books (BooksToScrape)**: Create a dataset of popular books in different genres by scraping the site *Books To Scrape*: http://books.toscrape.com\n",
    "\n",
    "\n",
    "7. **Dataset of Quotes (QuotesToScrape)**: Create a dataset of popular quotes for different tags by scraping the site *Quotes To Scrape*: http://quotes.toscrape.com\n",
    "\n",
    "\n",
    "8. **Scrape a User's Repositories (GitHub)**: Given someone's GitHub username, can you scrape their GitHub profile to create a list of their repositories with information like repository name, no. of stars, no. of forks, etc.?\n",
    "\n",
    "\n",
    "9. **Bibliography of an Author (Wikipedia)**: The list of books/publications by an author is called their bibliography. Here's an example bibliography page on Wikipedia: https://en.wikipedia.org/wiki/Charles_Dickens_bibliography . Can you scrape this information and create a dataset of [bibliographies](https://en.wikipedia.org/wiki/Category:Bibliographies_by_writer) for popular authors?\n",
    "\n",
    "\n",
    "10. **Country Demographics (Wikipedia)**: Wikipedia provides detailed demographics information for several countries e.g. https://en.wikipedia.org/wiki/Demographics_of_India . Can you scrape these pages to create a dataset of [demographics](https://en.wikipedia.org/wiki/Category:Demographics_by_country) for several countries containing information like population, density, life expectancy, fertility rate, infant mortality rate, age groups, etc.?\n",
    "\n",
    "\n",
    "11. **Stocks Prices (Yahoo Finance)**: Yahoo finance provides detailed information about stocks of publicly listed companies e.g. https://finance.yahoo.com/quote/TWTR . Can you scrape this information to create a dataset of stock prices for popular companies?\n",
    "\n",
    "\n",
    "12. **Create a Dataset of YouTube Videos (YouTube)**: Can you write a program to scrape information about videos from a YouTube channel page e.g. https://www.youtube.com/c/JovianML/videos ? Use this to create a dataset of top videos from popular channels.\n",
    "\n",
    "\n",
    "13. **Songs Dataset (AZLyrics)**: Create a dataset of songs by scraping AZLyrics: https://www.azlyrics.com/f.html . Capture information like song title, artist name, year of release and lyrics URL. \n",
    "\n",
    "\n",
    "14. **Scrape a Popular Blog**: Create a dataset of blog posts on a popular blog e.g. https://m.signalvnoise.com/search/ . The dataset can contain information like the blog title, published date, tags, author, link to blog post, etc.\n",
    "\n",
    "\n",
    "15. **Weekly Top Songs (Top 40 Weekly)**: Create a dataset of the top 40 songs of each week in a given year by scraping the site https://top40weekly.com . Capture information like song title, artist, weekly rank, etc.\n",
    "\n",
    "\n",
    "16. **Video Games Dataset (Steam)**: Create a dataset of popular or trending video games by scraping the listing pages on platforms like Steam: https://store.steampowered.com/genre/Free%20to%20Play/ .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
